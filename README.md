# ðŸŒ¿ Fine-Tuned E-commerce Chatbot for Ferns & Petals

## ðŸš€ Overview
This repository contains the fine-tuning process and implementation of an **e-commerce chatbot** specifically designed for **Ferns & Petals (FNP)**. The chatbot is trained using **Llama-2-7B** and optimized with **LoRA fine-tuning**, allowing it to provide **human-like, personalized, and proactive responses** to customer queries.

---

## ðŸ“Œ Features
âœ… **Pre-Trained LLM Fine-Tuning** â€“ Optimized using **TinyPixel/Llama-2-7B-bf16-sharded**  
âœ… **LoRA Optimization** â€“ Enhancing adaptability while reducing training costs  
âœ… **Custom Synthetic Dataset** â€“ Created with ChatGPT, available on **Hugging Face**  
âœ… **Realistic Customer Interactions** â€“ Handling order tracking, recommendations, & support  
âœ… **Efficient Training Pipeline** â€“ Using **BitsAndBytes quantization**, AdamW optimizer, and BLEU scoring  
âœ… **Personalization & Proactive Engagement** â€“ Future-ready for advanced NLU enhancements  

---

## ðŸ“‚ Dataset
The model was fine-tuned using a custom **synthetic dataset**:  
ðŸ“Œ **[deepsolv_ecommerce_chatbot](https://huggingface.co/datasets/monoid07/deepsolv_ecommerce_chatbot)**  
This dataset, generated with ChatGPT, includes **customer concerns** and corresponding **chatbot responses**, ensuring **realistic e-commerce conversations**.

---

## ðŸŽ¯ Model Selection & Training
### ðŸ”¹ **Why Llama-2-7B?**
- **Powerful text generation** for high-quality responses
- **Efficient performance** with LoRA fine-tuning & quantization

### ðŸ”¹ **Fine-Tuning Setup**
- **Quantization:** `bnb_4bit_quant_type="nf4"`, `fp16 training`
- **LoRA Configuration:** `lora_alpha=16`, `rank=8`
- **Trainer:** `SFTTrainer` from Hugging Face `transformers`
- **Optimizer:** AdamW (`paged_adamw_32bit`)
- **Batch Size:** `1 sequence per device`, Gradient Accumulation: `4`
- **Learning Rate:** `2e-4`, **Max Steps:** `100`

---

## ðŸ“Š Evaluation Metrics
To ensure high-quality responses, the following metrics were used:
- **BLEU Score** â€“ Evaluates text fluency & similarity
- **Perplexity (PPL)** â€“ Measures model confidence & coherence
- **Loss Monitoring** â€“ Tracks training efficiency with `PrintMetricsCallback`

---

## ðŸ¤– Enhanced Chatbot Capabilities
### ðŸ”¹ **Post-Fine-Tuning Enhancements**
âœ… **Improved Response Quality** â€“ More natural & contextual answers  
âœ… **Personalization** â€“ Laying the foundation for personalized customer engagement  
âœ… **Proactive Interactions** â€“ Predictive conversations for abandoned carts  
âœ… **Advanced NLU (Future Scope)** â€“ Enhanced intent recognition & response generation  

---



### ðŸ“Œ **Running the Chatbot**
```bash
python main.py
```



---

## ðŸ’¡ Future Improvements
- **Fine-tuning with real customer interactions**
- **Improved personalization with user history tracking**
- **Multilingual support for global reach**
- **Integration with FNPâ€™s existing chatbot systems**

---
